{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "occasional-response",
   "metadata": {},
   "source": [
    "# Cross Entropy Method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outer-patch",
   "metadata": {},
   "source": [
    "## Taxonomy of RL Methods\n",
    "All methods of RL can be classified in three types\n",
    "- Model free or Model based\n",
    "- value based or policy based\n",
    "- policy on or policy off\n",
    "\n",
    "**Model-Free** means that the method doesn't build a model of the environment or reward; it directly connects observations to actions. agent does some computations and selects the action based on that computation\n",
    "\n",
    "**Model-based** method try to predict what the next observation and/or reward will be.\n",
    "\n",
    "**policy based** methods directly approximates the policy of the agent ie what actions agent should carry out at every step. In this method we calculate probabilities of the action.\n",
    "\n",
    "**value-based** methods directly approximates the value of the action the agent might take and chooses the action with the best value.\n",
    "\n",
    "**policy on or off** means ability of agent to learn based on historical data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "associate-indianapolis",
   "metadata": {},
   "source": [
    "### Cross Entropy Method\n",
    "Our cross-entropy method is model-free, policy-based, and on-policy which means\n",
    "- It doesn't build any model of the environment; it just says to the agent what to do at every step\n",
    "- It approximates the policy of the agent\n",
    "- It requires fresh data obtained from the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "antique-albany",
   "metadata": {},
   "source": [
    "#### Practical cross-entropy\n",
    "![](policy_approximation_cross_entropy.png)\n",
    "\n",
    "In practice, policy is usually represented as probability distribution over actions, which makes it very similar to a classification problem, with the amount of classes being equal to amount of actions we can carry out. This abstraction makes our agent very simple: it needs to pass an observation from the environment to the network, get probability distribution over actions, and perform random sampling using probability distribution to get an action to carry out. This random sampling adds randomness to our agent, which is a good thing, as at the beginning of the training when our weights are random, the agent behaves randomly. After the agent gets an action to issue, it fires the action to the environment and obtains the next observation and reward for the last action. Then the loop continues.\n",
    "\n",
    "During the agent's lifetime, its experience is present as episodes. Every episode is a sequence of observations that the agent has got from the environment, actions it has issued, and rewards for these actions. Imagine that our agent has played several such episodes. For every episode, we can calculate the total reward that the agent has claimed. It can be discounted or not discounted, but for simplicity, let's assume a discount factor of gamma = 1, which means just a sum of all local rewards for every episode.\n",
    "\n",
    "![](Sample_episode.png)\n",
    "\n",
    "Every cell represents the agent's step in the episode. Due to randomness in the environment and the way that the agent selects actions to take, some episodes will be better than others. The core of the cross-entropy method is to throw away bad episodes and train on better ones. So, the steps of the method are as follows:\n",
    "\n",
    "1. Play N number of episodes using our current model and environment.\n",
    "2. Calculate the total reward for every episode and decide on a reward boundary. Usually, we use some percentile of all rewards, such as 50th or 70th.\n",
    "3. Throw away all episodes with a reward below the boundary.\n",
    "4. Train on the remaining \"elite\" episodes using observations as the input and issued actions as the desired output.\n",
    "5. Repeat from step 1 until we become satisfied with the result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "center-salem",
   "metadata": {},
   "source": [
    "### Cross Entropy Cartpole Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "champion-transfer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 16\n",
    "PERCENTILE = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "dying-paper",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "instrumental-transaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "Episode = namedtuple(\"Episode\", field_names = [\"reward\",\"steps\"])\n",
    "EpisodeStep = namedtuple(\"EpisodeStep\", field_names = [\"observation\", \"action\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "south-company",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_batches(env, net, batch_size):\n",
    "    batch = []\n",
    "    episode_reward = 0.0\n",
    "    episode_steps = []\n",
    "    obs = env.reset()\n",
    "    sm = nn.Softmax(dim=1)\n",
    "    while True:\n",
    "        obs_v = torch.FloatTensor([obs])\n",
    "        action_probs_v = sm(net(obs_v))\n",
    "        action_probs = action_probs_v.data.numpy()[0]\n",
    "        action = np.random.choice(len(action_probs), p = action_probs)\n",
    "        next_obs, reward, done, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "        episode_steps.append(EpisodeStep(observation = obs,action = action))\n",
    "        if done:\n",
    "            batch.append(Episode(reward = episode_reward,steps = episode_steps))\n",
    "            episode_reward = 0.0\n",
    "            episode_steps = []\n",
    "            next_obs = env.reset()\n",
    "            if len(batch) == batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "        obs = next_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "stable-steps",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_batches(batch, percentile):\n",
    "    rewards = list(map(lambda s:s.reward, batch))\n",
    "    reward_bound = np.percentile(rewards, percentile)\n",
    "    reward_mean = float(np.mean(rewards))\n",
    "    train_obs = []\n",
    "    train_act = []\n",
    "    for example in batch:\n",
    "        if example.reward < reward_bound:\n",
    "            continue\n",
    "        train_obs.extend(map(lambda steps:steps.observation, example.steps))\n",
    "        train_act.extend(map(lambda steps:steps.action, example.steps))\n",
    "    train_obs_v = torch.FloatTensor(train_obs)\n",
    "    train_act_v = torch.LongTensor(train_act)\n",
    "    return train_obs_v, train_act_v, reward_bound, reward_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "certain-permission",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 70\n",
      "0: loss=0.686, reward_mean=28.9, reward_bound=31.0\n",
      "1: loss=0.660, reward_mean=42.8, reward_bound=45.5\n",
      "2: loss=0.651, reward_mean=43.1, reward_bound=50.5\n",
      "3: loss=0.639, reward_mean=48.9, reward_bound=53.0\n",
      "4: loss=0.619, reward_mean=54.1, reward_bound=64.0\n",
      "5: loss=0.616, reward_mean=52.6, reward_bound=62.0\n",
      "6: loss=0.601, reward_mean=55.7, reward_bound=65.0\n",
      "7: loss=0.584, reward_mean=62.1, reward_bound=72.5\n",
      "8: loss=0.586, reward_mean=67.9, reward_bound=83.5\n",
      "9: loss=0.572, reward_mean=72.9, reward_bound=82.5\n",
      "10: loss=0.558, reward_mean=94.5, reward_bound=107.0\n",
      "11: loss=0.553, reward_mean=74.4, reward_bound=84.5\n",
      "12: loss=0.556, reward_mean=74.1, reward_bound=85.5\n",
      "13: loss=0.541, reward_mean=102.0, reward_bound=121.5\n",
      "14: loss=0.546, reward_mean=117.2, reward_bound=143.0\n",
      "15: loss=0.542, reward_mean=108.8, reward_bound=127.5\n",
      "16: loss=0.522, reward_mean=143.8, reward_bound=190.0\n",
      "17: loss=0.517, reward_mean=137.1, reward_bound=150.5\n",
      "18: loss=0.531, reward_mean=144.7, reward_bound=160.0\n",
      "19: loss=0.523, reward_mean=148.1, reward_bound=184.0\n",
      "20: loss=0.521, reward_mean=166.2, reward_bound=193.0\n",
      "21: loss=0.508, reward_mean=160.3, reward_bound=200.0\n",
      "22: loss=0.518, reward_mean=176.9, reward_bound=200.0\n",
      "23: loss=0.510, reward_mean=196.2, reward_bound=200.0\n",
      "24: loss=0.513, reward_mean=195.2, reward_bound=200.0\n",
      "25: loss=0.508, reward_mean=198.2, reward_bound=200.0\n",
      "26: loss=0.503, reward_mean=199.9, reward_bound=200.0\n",
      "Solved!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"CartPole-v0\")\n",
    "    # env = gym.wrappers.Monitor(env, directory=\"mon\", force=True)\n",
    "    obs_size = env.observation_space.shape[0]\n",
    "    n_actions = env.action_space.n\n",
    "\n",
    "    net = Net(obs_size, HIDDEN_SIZE, n_actions)\n",
    "    objective = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(params=net.parameters(), lr=0.01)\n",
    "    writer = SummaryWriter(comment=\"-cartpole\")\n",
    "    print(BATCH_SIZE, PERCENTILE)\n",
    "    for iter_no, batch in enumerate(iterate_batches(env, net, BATCH_SIZE)):\n",
    "        obs_v, actions_v, reward_b, reward_m = filter_batches(batch, PERCENTILE)\n",
    "        optimizer.zero_grad()\n",
    "        action_scores_v = net(obs_v)\n",
    "        loss_v = objective(action_scores_v, actions_v)\n",
    "        loss_v.backward()\n",
    "        optimizer.step()\n",
    "        print(\"%d: loss=%.3f, reward_mean=%.1f, reward_bound=%.1f\" % (\n",
    "            iter_no, loss_v.item(), reward_m, reward_b))\n",
    "        writer.add_scalar(\"loss\", loss_v.item(), iter_no)\n",
    "        writer.add_scalar(\"reward_bound\", reward_b, iter_no)\n",
    "        writer.add_scalar(\"reward_mean\", reward_m, iter_no)\n",
    "        if reward_m > 199:\n",
    "            print(\"Solved!\")\n",
    "            break\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changing-string",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
