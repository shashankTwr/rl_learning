{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "undefined-perth",
   "metadata": {},
   "source": [
    "# Tabular Learning and the Bellman Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlikely-growth",
   "metadata": {},
   "source": [
    "## Bellman equation of optimality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colored-system",
   "metadata": {},
   "source": [
    "### Value of State\n",
    " \n",
    " It is represented by ${V_s}$. It means the expected total reward that we can get from the state. \n",
    " \n",
    " ${V_s} = {\\mathbb{E}}{[{\\sum\\limits_{t=0}^{\\infty}}{(r_{t}  \\gamma_{t})}]}$, \n",
    " \n",
    " \n",
    " where ${r_t}$ is the local reward obtained at the time step t of the episode. The total reward could be discounted or not; it's up to us how to define it. Value is always calculated in the respect of some policy that our agent follows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tender-sense",
   "metadata": {},
   "source": [
    "# Value of Action\n",
    "\n",
    "${Q_s,_a} = \\mathbb{E}_{x' ~ x}[r_s,_a + {\\gamma}{V_s'}] = {\\sum\\limits_{{s'}\\in S}}\n",
    "{\\mathcal{p}_{a,s \\rightarrow s'}}{({r_s,_a} + {\\gamma}{V_{s'}})}$\n",
    "\n",
    "We can also define  ${V_s}$ via  ${Q_s,_a}$\n",
    "\n",
    "${V_s} = {\\max\\limits_{a\\in A}}{Q_s,_a}$, \n",
    "\n",
    "hence we can extend ${Q_s,_a}$ and express it by itself \n",
    "\n",
    "${Q_s,_a} = {r_s,_a} + {\\gamma}    {\\max\\limits_{a'\\in A}}{Q_s',a'}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consolidated-fountain",
   "metadata": {},
   "source": [
    "What is a value of action? \n",
    "\n",
    "To make our life slightly easier, we can define different quantities in addition to the value of state : value of action . Basically, it equals the total reward we can get by executing action a in state s and can be defined via . Being a much less fundamental entity than , this quantity gave a name to the whole family of methods called **\"Q-learning\"**, because it is slightly more convenient in practice. In these methods, ourÂ primary objective is to get values of Q for every pair of state and action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pregnant-superintendent",
   "metadata": {},
   "source": [
    "### Value iteration method\n",
    "\n",
    "#### For ${V_s}$\n",
    "\n",
    "1. Initialize initial values of states s (usually set to zero)\n",
    "2. Perform the Bellman updated on the MDP for ${V_s}$\n",
    "\n",
    "    ${V_s} \\leftarrow {\\max_a}{\\sum_{s'}} {\\mathcal{p}_{s,a \\rightarrow s'}}{(r_{s,a} + {\\gamma}{V_s'})}$\n",
    "3. Repeat step 2 for a large number of iterations untill the change becomes negligible\n",
    "\n",
    "#### For ${Q_{s,a}}$\n",
    "1. Initialize initial values of states s (usually set to zero)\n",
    "2. Perform the Bellman updated on the MDP for ${Q_{s,a}}$\n",
    "\n",
    "    ${Q_{s,a}} \\leftarrow {\\max_a}{\\sum_{s'}} {\\mathcal{p}_{s,a \\rightarrow s'}}{(r_{s,a} + {\\gamma}{\\max_a'}{Q_{s',a'}})}$\n",
    "3. Repeat step 2 for a large number of iterations untill the change becomes negligible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equal-portrait",
   "metadata": {},
   "source": [
    "### Limitations of Value Iteration\n",
    "\n",
    "1. Our state space should be discrete and small enough to perform multiple iterations over all states\n",
    "2. The second practical problem arises from the fact that we rarely know the transition probability for the actions and rewards matrix\n",
    "\n",
    "To tackle this issue, we need to keep track of agent experience and then maintain counters for every tuple ${({s_o},{s_1},{a_0})}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
