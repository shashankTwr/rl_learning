{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "essential-haven",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  An input in DL can be represented as \n",
    "# NUMBER, VECTOR, MATRIX\n",
    "# a tensor could be used to represent all the above\n",
    "# tensor is basically n-Dimensional object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "lightweight-camel",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Pytorch has 8 types of tensor: 3 float 5 integer\n",
    "# float 16 32 64\n",
    "# integer 8 signed, 8 unsigned, 16,32,64 bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "hidden-bulgaria",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tensor can be created by three ways\n",
    "#  calling a constructror of required type\n",
    "# converting a numpy array into tensor\n",
    "# using torch.zeros function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "floral-separate",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "endless-constitutional",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.FloatTensor(3,2)\n",
    "a.zero_()\n",
    "n= np.zeros(shape=(3,3))\n",
    "torch.tensor(n, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "floral-device",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using double precision usually causes memory and performance overhead\n",
    "#  double precision meaning using 64bits float tensor type "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "celtic-novelty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch transparently supports CUDA GPUs,\n",
    "# which means that all operations have two versions—CPU and GPU—which are automatically selected.\n",
    "# The decision is made based on the type of tensors that you are operating on. \n",
    "# Every tensor type that we mentioned is for CPU and has its GPU equivalent.\n",
    "# The only difference is that GPU tensors reside in the torch.cuda package,\n",
    "# instead of just torch. \n",
    "# For example, torch.FloatTensor is a 32-bit float tensor which resides in CPU memory, \n",
    "# but torch.cuda.FloatTensor is its GPU counterpart.\n",
    "# To convert from CPU to GPU, there is a tensor method, **to(device)**, \n",
    "# which creates a copy of the tensor to a specified device (which could be CPU or GPU).\n",
    "# If the tensor is already on the device, nothing happens and the original tensor will be returned.\n",
    "# Device type can be specified in different ways. \n",
    "# 1) you can just pass a string name of the device, which is \"cpu\" for CPU memory or \"cuda\" for GPU. \n",
    "#   A GPU device could have an optional device index specified after the colon, \n",
    "#     for example, the second GPU card in the system could be \n",
    "#     addressed by \"cuda:1\" (index is zero-based)\n",
    "# 2)  specify a device in the to() method is using the torch.device class,\n",
    "# which accepts the device name and optional index.\n",
    "# For accessing the device that your tensor is currently residing in, it has a device property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "focused-verse",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ca = a.cuda()\n",
    "ca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "educational-ministry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1.],\n",
       "         [1., 1.],\n",
       "         [1., 1.]]),\n",
       " tensor([[1., 1.],\n",
       "         [1., 1.],\n",
       "         [1., 1.]], device='cuda:0'),\n",
       " device(type='cuda', index=0))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a+1,ca+1,ca.device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visible-geology",
   "metadata": {},
   "source": [
    "# Gradients:\n",
    "Gradients can be calculated two different ways:\n",
    "1) Static 2) Dynamic\n",
    "\n",
    "1) *Static*: Calculations are need to be defined in advanced and no more changes are possible. Graph gets processed and optimized by the DL library before any computation can be made.\n",
    "\n",
    "2) *Dynamic*: You don't need to define your graph in advance exactly as it will be executed. You just execute operations that you want to use for data transformation on your actual data. During this, the library records the order of operations performed, and when you ask it to calculate gradients, it unrolls its history of operations, accumulating the gradients of network parameters.This method is called **notebook gradients**\n",
    "\n",
    "\n",
    "In static graph, the library has much more freedom in optimizing the order that computations are performed in or even removing parts of the graph. On the other hand, dynamic graph has a higher computation overhead, but gives a developer much more freedom. For example, they can say, \"For this piece of data, I can apply this network two times, and for this piece of data, I'll use a completely different model with gradients clipped by the batch mean.\" Another very appealing strength of the dynamic graph model is that it allows you to express your transformation more naturally, in a more \"Pythonic\" way. In the end, it's just a Python library with bunch of functions, so just call them and let the library do the magic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "differential-retail",
   "metadata": {},
   "source": [
    "# Tensors and Gradients\n",
    "\n",
    "there are several attributes related to gradients that every tensor has:\n",
    "- grad: A property which holds a tensor of the same shop containing computed gradients.\n",
    "- is_leaf: *True*, if this tensor was constructed by the user and *False*, if the object is a result of a function transformation.\n",
    "- requires_grad: *True* if this tensor requires gradients to be calculated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "happy-counter",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "<img src=\"./image/summation_image.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "twelve-master",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of such\n",
    "v1 = torch.tensor([1.0, 1.0], requires_grad=True)\n",
    "v2 = torch.tensor([2.0, 2.0])\n",
    "v_sum = v1 + v2\n",
    "v_res = (v_sum*2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "patent-technician",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1.], requires_grad=True),\n",
       " tensor([2., 2.]),\n",
       " tensor([3., 3.], grad_fn=<AddBackward0>),\n",
       " tensor(12., grad_fn=<SumBackward0>))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1,v2,v_sum,v_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "acoustic-screen",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, False, False)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if they are leaf(meaning constructed by the user)\n",
    "v1.is_leaf,v2.is_leaf,v_sum.is_leaf,v_res.is_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "willing-garlic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, False, True, True)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if these tensors requires gradients\n",
    "v1.requires_grad, v2.requires_grad, v_sum.requires_grad, v_res.requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "typical-grade",
   "metadata": {},
   "source": [
    "## NN Building blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olive-softball",
   "metadata": {},
   "source": [
    "### Basic example\n",
    "we created a randomly initialized feed-forward layer, with two inputs and five outputs, and applied it to our float tensor. All classes in the torch.nn packages inherit from the nn.Module base class, which you can use to implement your own higher-level NN blocks\n",
    "\n",
    "Useful methods that all nn.Module children provide. They are as follows:\n",
    "\n",
    "- **parameters()**: A function that returns iterator of all variables which require gradient computation (that is, module weights)\n",
    "- **zero_grad()**: This function initializes all gradients of all parameters to zero\n",
    "to(device): This moves all module parameters to a given device (CPU or GPU)\n",
    "- **state_dict()**: This returns the dictionary with all module parameters and is useful for model serialization\n",
    "- **load_state_dict()**: This initializes the module with the state dictionary\n",
    "\n",
    "\n",
    "The whole list of available classes can be found in the documentation at http://pytorch.org/docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "preliminary-haiti",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.6061, -0.9103,  1.8675, -0.1647,  0.4338], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "l = nn.Linear(2,5)\n",
    "v = torch.FloatTensor([1,2])\n",
    "l(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "royal-headquarters",
   "metadata": {},
   "source": [
    "**Sequential** class allows you to combine other layers into the pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "nervous-eclipse",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=2, out_features=5, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=5, out_features=20, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=20, out_features=10, bias=True)\n",
       "  (5): Dropout(p=0.3, inplace=False)\n",
       "  (6): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = nn.Sequential(nn.Linear(2,5),nn.ReLU(),\n",
    "                  nn.Linear(5,20),nn.ReLU(),\n",
    "                  nn.Linear(20,10),nn.Dropout(p=0.3),nn.Softmax(dim=1))\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governing-grace",
   "metadata": {},
   "source": [
    "## Custom Layers\n",
    "By subclassing the **nn.Module** class, you can create your own building blocks which can be stacked together, reused later, and integrated into the PyTorch framework flawlessly.\n",
    "At its core, nn.Module provides quite rich functionality to its children:\n",
    "\n",
    "- It tracks all submodules that the current module includes. For example, your building block can have two feed-forward layers used somehow to perform the block's transformation.\n",
    "- It provides functions to deal with all parameters of the registered submodules. You can obtain a full list of the module's parameters (parameters() method), zero its gradients (zero_grads() method), move to CPU or GPU (to(device) method), serialize and deserialize the module (state_dict() and load_state_dict()), and even perform generic transformations using your own callable (apply() method).\n",
    "- It establishes the convention of module application to data. Every module needs to perform its data transformation in the forward() method by overriding it.\n",
    "- There are some more functions, such as the ability to register a hook function to tweak module transformation or gradients flow, but it's more for advanced use cases.\n",
    "\n",
    "In **PyTorch**, to create a custom module, we usually have to do only two things: \n",
    "- register submodules \n",
    "- implement the forward() method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "historic-government",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurModule(nn.Module):\n",
    "    def __init__(self, num_inputs, num_classes, dropout_prob=0.3):\n",
    "        super(OurModule, self).__init__()\n",
    "        self.pipe = nn.Sequential(\n",
    "            nn.Linear(num_inputs, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5, 20),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(20, num_classes),\n",
    "            nn.Dropout(p=dropout_prob),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.pipe(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "piano-visiting",
   "metadata": {},
   "source": [
    "### Explaining above example\n",
    "This is our module class that inherits nn.Module. In the constructor, we pass three parameters: the size of input, size of output, and optional dropout probability. The first thing we need to do is to call the parent's constructor to let it initialize itself. In the second step, we create an already familiar nn.Sequential with a bunch of layers and assign it to our class field named pipe. By assigning a Sequential instance to our field, we automatically register this module (nn.Sequential inherits from nn.Module as does everything in the nn package). To register, we don't need to call anything, we just assign our submodules to fields. After the constructor finishes, all those fields will be registered automatically (if you really want to, there is a function in nn.Module to register submodules)\n",
    "\n",
    "Here, we override the forward function with our implementation of data transformation. As our module is a very simple wrapper around other layers, we just need to ask them to transform the data. Note that to apply a module to the data, you need to call the module as callable (that is, pretend that the module instance is a function and call it with the arguments) and not use the forward() function of the nn.Module class. This is because nn.Module overrides the __call__() method, which is being used when we treat an instance as callable. This method does some nn.Module magic stuff and calls your forward() method. **If you call forward() directly, you'll intervene with the nn.Module duty, which can give you wrong results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "technological-metabolism",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-3df5b9d6b63d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOurModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rlprac/openai/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rlprac/openai/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_forward_unimplemented\u001b[0;34m(self, *input)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mregistered\u001b[0m \u001b[0mhooks\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlatter\u001b[0m \u001b[0msilently\u001b[0m \u001b[0mignores\u001b[0m \u001b[0mthem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \"\"\"\n\u001b[0;32m--> 175\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    net = OurModule(num_inputs=2, num_classes=3)\n",
    "    v = torch.FloatTensor([[2,3]])\n",
    "    out = net(v)\n",
    "    print(net)\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amber-sister",
   "metadata": {},
   "source": [
    "Above code is to check execution and see how our custom module looks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cubic-henry",
   "metadata": {},
   "source": [
    "### Loss Function and Loss value\n",
    "We need to define our learning objective, which is to have a function that accepts two arguments: the network's output and the desired output. Its responsibility is to return to us a single number: how close the network's prediction is from the desired result. This function is called the **loss function**, and its output is the **loss value**. Using the loss value, we calculate gradients of network parameters and adjust them to decrease this loss value, which pushes our model to better results in the future. Both of those pieces—the loss function and the method of tweaking a network's parameters by gradients—are so common and exist in so many forms that both of them form a significant part of the PyTorch library. Let's start with loss functions.\n",
    "\n",
    "Loss functions reside in the nn package and are implemented as an nn.Module subclass. Usually, they accept two arguments: output from the network (prediction), and desired output (ground-truth data which is also called the label of the data sample). At the time of writing, PyTorch 0.4 contains 17 different loss functions\n",
    "\n",
    "The most commonly used are:\n",
    "\n",
    "- **nn.MSELoss**: The mean square error between arguments, which is the standard loss for regression problems\n",
    "- **nn.BCELoss and nn.BCEWithLogits**: Binary cross-entropy loss. The first version expects a single probability value (usually it's the output of the Sigmoid layer), while the second version assumes raw scores as input and applies Sigmoid itself. The second way is usually more numerically stable and efficient. These losses (as their names suggest) are frequently used in binary classification problems.\n",
    "- **nn.CrossEntropyLoss and nn.NLLLoss**: Famous \"maximum likelihood\" criteria, which is used in multi-class classification problems. The first version expects raw scores for each class and applies LogSoftmax internally, while the second expects to have log probabilities as the input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strange-flashing",
   "metadata": {},
   "source": [
    "### Optimizers\n",
    "\n",
    "The responsibility of the basic optimizer is to take gradients of model parameters and change these parameters, in order to decrease loss value. By decreasing loss value, we're pushing our model towards desired outputs, which can give us hope of better model performance in the future. \n",
    "PyTorch provides lots of popular optimizer implementations and the most widely known are as follows:\n",
    "\n",
    "- **SGD**: A vanilla stochastic gradient descent algorithm with optional momentum extension\n",
    "- **RMSprop**: An optimizer, proposed by G. Hinton\n",
    "- **Adagrad**: An adaptive gradients optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exposed-moscow",
   "metadata": {},
   "source": [
    "## Monitoring NN\n",
    "\n",
    "DL practitioners have developed a list of things that you should observe during your training, which usually includes the following:\n",
    "\n",
    "- Loss value, which normally consists of several components like base loss and regularization losses. You should monitor both total loss and individual components over time.\n",
    "- Results of validation on training and test sets.\n",
    "- Statistics about gradients and weights.\n",
    "- Learning rates and other hyperparameters, if they are adjusted over time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bottom-dragon",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    funcs = {\"sin\": math.sin, \"cos\": math.cos, \"tan\": math.tan}\n",
    "    for angle in range(-360, 360):\n",
    "        angle_rad = angle * math.pi / 180\n",
    "        for name, fun in funcs.items():\n",
    "            val = fun(angle_rad)\n",
    "            writer.add_scalar(name, val, angle)\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interpreted-passion",
   "metadata": {},
   "source": [
    "command to run the writer\n",
    "\n",
    "tensorboard --logdir runs --host localhost"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
